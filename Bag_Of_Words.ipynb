{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ewilwertuoz/Work-with-Gensim-and-NLTK/blob/main/Bag_Of_Words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "id": "MNP39qmPREUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nmslib"
      ],
      "metadata": {
        "id": "by52KntdpagH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "bGwOOEfnijIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M05bYV2fSxfy"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import gensim\n",
        "import fitz\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.similarities.nmslib import NmslibIndexer\n",
        "from gensim.models import Word2Vec\n",
        "from tempfile import mkstemp\n",
        "from gensim import corpora\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DWGPKF_ET1X1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1261e38c-5db2-410e-f967-7ee627d3de78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")"
      ],
      "metadata": {
        "id": "gyE2noJb2SOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "ps = PorterStemmer()\n",
        "wnl = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "no9ncSRDoudP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/books/\""
      ],
      "metadata": {
        "id": "SpDrMnUHLJi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(path + \"all_file.txt\", \"r\") as file:\n",
        "    all_files = file.readlines()\n",
        "file.close()\n",
        "\n",
        "all_txt = []\n",
        "for i in range(len(all_files)):\n",
        "  all_files[i] = all_files[i].replace(\"\\n\", \"\")\n",
        "  all_txt.append(all_files[i].replace(\".pdf\", \".txt\"))"
      ],
      "metadata": {
        "id": "fGrEH7Fje480"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pdf(directory):\n",
        "  doc = fitz.open(directory)\n",
        "  txt = \"\"\n",
        "  for current_page in range(len(doc)):\n",
        "    page = doc.load_page(current_page)\n",
        "    if current_page == 0:\n",
        "      txt += page.get_text(\"text\")\n",
        "    else:\n",
        "      txt += \"\\f\" + page.get_text(\"text\")\n",
        "  txt = txt.replace(\"-\\r\", \"\").replace(\"-\\n\", \"\")\n",
        "  return txt"
      ],
      "metadata": {
        "id": "l3ulSHAXUpKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_lemma(sent):\n",
        "  words = []\n",
        "  words = [wnl.lemmatize(word) for word in simple_preprocess(sent, deacc=True)]\n",
        "  return words\n",
        "\n",
        "def add_stem(sent):\n",
        "  words = []\n",
        "  words = [ps.stem(word) for word in simple_preprocess(sent, deacc=True)]\n",
        "  return words"
      ],
      "metadata": {
        "id": "tpSK04Hfksy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = []\n",
        "\n",
        "rev_my_dictionary = {}\n",
        "\n",
        "for file in all_files:\n",
        "  text = read_pdf(path + file)\n",
        "  for sentence in text.split('.'):\n",
        "    tokenized.append(simple_preprocess(sentence, deacc = True))\n",
        "\n",
        "  my_dictionary = corpora.Dictionary(tokenized)\n",
        "\n",
        "  for key in my_dictionary.values():\n",
        "    st_key = ps.stem(key)\n",
        "    if st_key in rev_my_dictionary:\n",
        "      if key not in rev_my_dictionary.get(st_key):\n",
        "        rev_my_dictionary.get(st_key).append(key)\n",
        "    else:\n",
        "      rev_my_dictionary[ps.stem(key)] = [key]\n",
        "\n",
        "tokenized.clear()"
      ],
      "metadata": {
        "id": "ihAzZG1omvkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = []\n",
        "\n",
        "for file in all_files:\n",
        "  text = read_pdf(path + file)\n",
        "  for sentence in text.split('.'):\n",
        "      tokenized.append(add_stem(sentence))"
      ],
      "metadata": {
        "id": "sBrZXF-K50kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(tokenized, min_count=1, epochs=100, seed=2)\n",
        "indexer = NmslibIndexer(model)\n",
        "_, temp_fn = mkstemp()\n",
        "indexer.save(temp_fn)\n",
        "new_indexer = NmslibIndexer.load(temp_fn)"
      ],
      "metadata": {
        "id": "Ld6P2QQv2gnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(item):\n",
        "  elem = {}\n",
        "  text = \"\"\n",
        "  for txt in all_txt:\n",
        "    nums = 0\n",
        "    with open(path + \"files/\" + txt, \"r\") as fl:\n",
        "      text = fl.read()\n",
        "      fl.close()\n",
        "    pages = text.split(\"\\f\")\n",
        "    for page in pages:\n",
        "      nums += 1\n",
        "      if item in page:\n",
        "        if txt.replace(\".txt\", \"\") in elem:\n",
        "          elem.get(txt.replace(\".txt\", \"\")).append(nums)\n",
        "        else:\n",
        "          elem[txt.replace(\".txt\", \"\")] = [nums]\n",
        "  return elem"
      ],
      "metadata": {
        "id": "zhU-KTn3bA6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find = \"intelligence\"\n",
        "stem_find = ps.stem(find)\n",
        "all = model.wv.most_similar(stem_find, topn=10, indexer=new_indexer)\n",
        "for i in range(len(all)):\n",
        "  print(f\"{all[i][0]} {all[i][1]}:\")\n",
        "  for item in rev_my_dictionary[all[i][0]]:\n",
        "    dic = search(item)\n",
        "    print(f\"\\t{item}:\")\n",
        "    for key in dic.keys():\n",
        "      print(f\"\\t\\t{key}: {dic.get(key)}\")"
      ],
      "metadata": {
        "id": "cWkAC5LBQEpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "file_names = os.listdir(path)\n",
        "\n",
        "with open(path + \"all_file.txt\", \"w\") as file:\n",
        "  for file_name in file_names:\n",
        "      if file_name == \".ipynb_checkpoints\" or file_name == \"all_file.txt\" or file_name == \"files\":\n",
        "        continue\n",
        "      file.write(file_name + \"\\n\")\n",
        "file.close()"
      ],
      "metadata": {
        "id": "1A0oCDuNamv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file in all_files:\n",
        "  text = read_pdf(path + file)\n",
        "  with open(path + \"files/\" + file.replace(\".pdf\", \".txt\"), \"w\") as fl:\n",
        "    fl.write(text)"
      ],
      "metadata": {
        "id": "kNMscZ09f6hZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}